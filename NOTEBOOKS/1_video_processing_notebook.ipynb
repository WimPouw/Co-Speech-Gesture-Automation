{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a077198",
   "metadata": {},
   "source": [
    "# <strong><u>1: Video Processing Notebook |</u></strong>\n",
    "\n",
    "This notebook is an implementation of video processing using MediaPipe, Google's pose estimation model. The process extracts x, y movement of the body using markerless motion capture for ## articulators, or keypoints. \n",
    "\n",
    "In this workflow, we output the x,y movement for the keypoints in the upper body that are particularly relevant for co-speech gesture research: `right_shoulder`, `left_shoulder`, `right_elbow`, `left_elbow`,  `right_wrist`,  `left_wrist`, `right_eye`,  `left_eye`, `nose`. It is possible to track other keypoints, as is documented in more detail in the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker). For this workflow, later stages of processing will further subset these keypoints for a specific analysis.\n",
    "\n",
    "This script takes a video input file, uses the MediaPipe model to track body movement, and outputs a dataframe with the time scale and x, y coordinates for the upperbody articulators specified above. The dataframe is then written to a csv file.\n",
    "\n",
    "This script assumes that you have cloned the Co-Speech-Gesture-Automation github repository and that your input files are stored in the VIDEO_FILES folder. Your outputs will be written to the MOTION_TRACKING_FILES folder.\n",
    "\n",
    "### <strong>Requirements</strong>\n",
    "\n",
    "To run this notebook, you will need the following Python packages:\n",
    "\n",
    "- mediapipe\n",
    "- opencv-python\n",
    "- pandas\n",
    "- matplotlib\n",
    "\n",
    "You can install these packages using pip:\n",
    "```shell\n",
    "    pip install mediapipe opencv-python pandas matplotlib\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00089a3",
   "metadata": {},
   "source": [
    "\n",
    "### <strong>Importing Libraries</strong>\n",
    "Here, we import essential libraries:\n",
    "- `pandas`: For DataFrame manipulation and data analysis\n",
    "- `numpy`: For numerical computations\n",
    "- `plotly`: For data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca871f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed58070",
   "metadata": {},
   "source": [
    "### <strong>Setting Parameters</strong>\n",
    "In this section, you can modify the following parameters:\n",
    "- `MODEL`: Choose between Lite model (`1`) and Full model (`2`). Lite Model (`1`) is the  `Default`. \n",
    "\n",
    "The lite model is faster and relatively accurate, but using the full model will improve tracking accuracy.\n",
    "\n",
    "- `video_path`: Path to the video file.\n",
    "\n",
    "\n",
    "This cell also provides you a way to check that the file and directory specified in `video_path` can be found before beginning video processing. If the file path was found, you can proceed to the video processing initialization and video processing steps. If your file was not found and you should amend your `video_path` before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "056b65dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n/kfranich_speech_l3/Lab/6000_EMA/video_and_audio_data/SN6001_ARTGEST_ENG_VIDAUD/SN6001_ARTGEST_ENG_VIDAUD_GEST/SN6001_ARTGEST_ENG_G_CVC_MONO.mp4 is a valid file. Proceed with processing.\n"
     ]
    }
   ],
   "source": [
    "MODEL = 2  # 1 = Lite model, 2 = Full model\n",
    "video_path = \"/n/kfranich_speech_l3/Lab/6000_EMA/video_and_audio_data/SN6001_ARTGEST_ENG_VIDAUD/SN6001_ARTGEST_ENG_VIDAUD_GEST/SN6001_ARTGEST_ENG_G_CVC_MONO.mp4\"\n",
    "video_extensions = [\"*.mp4\", \"*.avi\", \"*.MOV\"]\n",
    "\n",
    "if os.path.exists(video_path) == True:\n",
    "    print(f\"{video_path} is a valid file. Proceed with processing.\")\n",
    "else:\n",
    "    raise ValueError(f\"{video_path} does not exist. Try adding the entire file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1e2c4",
   "metadata": {},
   "source": [
    "### <strong>Initialization</strong>\n",
    "This part initializes MediaPipe components used in the notebook.\n",
    "\n",
    "There is no need to do anything with this cell before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34058d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe components\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3130a9",
   "metadata": {},
   "source": [
    "### <strong>Keypoint Tracking</strong>\n",
    "The core logic of video processing is performed in this loop.\n",
    "This code block reads in a video file and extracts pose landmarks from each frame of the video using the `mediapipe` and `opencv-python` libraries. The pose landmarks are then stored in a DataFrame along with the corresponding time stamp.\n",
    "\n",
    "This cell can be run without making any changes unless you require tracking for keypoints not specified in this workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baa76350",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with mp_holistic.Holistic(static_image_mode=False, model_complexity=MODEL, \n",
    "                          min_detection_confidence=0.4, min_tracking_confidence=0.4) as holistic:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = holistic.process(image)\n",
    "        time_ms = frame_count / fps\n",
    "\n",
    "        # Append pose landmarks data\n",
    "        if results.pose_landmarks is not None:\n",
    "            right_shoulder_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER].x\n",
    "            right_shoulder_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER].y\n",
    "            left_shoulder_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER].x\n",
    "            left_shoulder_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER].y\n",
    "            right_elbow_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW].x\n",
    "            right_elbow_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW].y\n",
    "            left_elbow_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW].x\n",
    "            left_elbow_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW].y\n",
    "            right_wrist_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_WRIST].x\n",
    "            right_wrist_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_WRIST].y\n",
    "            left_wrist_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_WRIST].x\n",
    "            left_wrist_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_WRIST].y\n",
    "            right_eye_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_EYE].x\n",
    "            right_eye_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_EYE].y\n",
    "            left_eye_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EYE].x\n",
    "            left_eye_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EYE].y\n",
    "            nose_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].x\n",
    "            nose_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].y\n",
    "            right_index_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_INDEX].x\n",
    "            right_index_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_INDEX].y\n",
    "\n",
    "            data.append([time_ms, right_shoulder_x, right_shoulder_y, left_shoulder_x, left_shoulder_y, \n",
    "                         right_elbow_x, right_elbow_y, left_elbow_x, left_elbow_y, \n",
    "                         right_wrist_x, right_wrist_y, left_wrist_x, left_wrist_y, \n",
    "                         right_eye_x, right_eye_y, left_eye_x, left_eye_y, right_index_x, right_index_y,\n",
    "                         nose_x, nose_y])\n",
    "\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cffe5c1",
   "metadata": {},
   "source": [
    "### <strong>Converting the data to a dataframe</strong>\n",
    "Here the keypoint data is converted to a dataframe to preview before writing to a csv file. All keypoint data are saved at this stage of processing, and there is no need to make changes to the articulators listed here; however, later stages of processing will subset the data to a chosen articulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ada8edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\n",
    "    \"time_ms\", \n",
    "    \"right_shoulder_x\", \"right_shoulder_y\", \n",
    "    \"left_shoulder_x\", \"left_shoulder_y\", \n",
    "    \"right_elbow_x\", \"right_elbow_y\", \n",
    "    \"left_elbow_x\", \"left_elbow_y\", \n",
    "    \"right_wrist_x\", \"right_wrist_y\", \n",
    "    \"left_wrist_x\", \"left_wrist_y\", \n",
    "    \"right_eye_x\", \"right_eye_y\", \n",
    "    \"left_eye_x\", \"left_eye_y\",\n",
    "    \"nose_x\", \"nose_y\",\n",
    "    \"right_index_x\", \"right_index_y\" \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691108d0",
   "metadata": {},
   "source": [
    "### <strong>Data Output</strong>\n",
    "Finally, the processed data is stored in a DataFrame and saved as a pickle file or csv file. For testing we recommend using a csv file and then moving to a pickle file for processing larger datasets because pickle files are more efficient for processing and storing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a626cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as /n/home_fasse/wdych/KEYPOINTS/SN6001_ARTGEST_ENG_G_CVC_MONO_keypoints.csv\n"
     ]
    }
   ],
   "source": [
    "# Save DataFrame as CSV file\n",
    "csv_file_name = \"/n/home_fasse/wdych/KEYPOINTS/\" + os.path.splitext(os.path.basename(video_path))[0] + \"_keypoints.csv\"\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "print(f\"DataFrame saved as {csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d293124d-601b-4768-ac89-049e42830b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
