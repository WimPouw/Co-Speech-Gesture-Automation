{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a077198",
   "metadata": {},
   "source": [
    "# <strong><u>1: Video Processing Notebook |</u></strong>\n",
    "\n",
    "This notebook is an implementation of video processing using MediaPipe, Google's pose estimation model. The process extracts x, y movement of the body using markerless motion capture for ## articulators, or keypoints. \n",
    "\n",
    "In this workflow, we output the x,y movement for the keypoints in the upper body that are particularly relevant for co-speech gesture research: `right_shoulder`, `left_shoulder`, `right_elbow`, `left_elbow`,  `right_wrist`,  `left_wrist`, `right_eye`,  `left_eye`, `nose`. It is possible to track other keypoints, as is documented in more detail in the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker). For this workflow, later stages of processing will further subset these keypoints for a specific analysis.\n",
    "\n",
    "This script takes a video input file, uses the MediaPipe model to track body movement, and outputs a dataframe with the time scale and x, y coordinates for the upperbody articulators specified above. The dataframe is then written to a csv file.\n",
    "\n",
    "This script assumes that you have cloned the Co-Speech-Gesture-Automation github repository and that your input files are stored in the VIDEO_FILES folder. Your outputs will be written to the MOTION_TRACKING_FILES folder.\n",
    "\n",
    "### <strong>Requirements</strong>\n",
    "\n",
    "To run this notebook, you will need the following Python packages:\n",
    "\n",
    "- mediapipe\n",
    "- opencv-python\n",
    "- pandas\n",
    "- matplotlib\n",
    "\n",
    "You can install these packages using pip:\n",
    "```shell\n",
    "    pip install mediapipe opencv-python pandas matplotlib\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00089a3",
   "metadata": {},
   "source": [
    "\n",
    "### <strong>Importing Libraries</strong>\n",
    "Here, we import essential libraries:\n",
    "- `pandas`: For DataFrame manipulation and data analysis\n",
    "- `numpy`: For numerical computations\n",
    "- `plotly`: For data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca871f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed58070",
   "metadata": {},
   "source": [
    "### <strong>Setting Parameters</strong>\n",
    "In this section, you can modify the following parameters:\n",
    "- `MODEL`: Choose between Lite model (`1`) and Full model (`2`). Lite Model (`1`) is the  `Default`. \n",
    "\n",
    "The lite model is faster and relatively accurate, but using the full model will improve tracking accuracy.\n",
    "\n",
    "- `video_path`: Path to the video file.\n",
    "\n",
    "\n",
    "This cell also provides you a way to check that the file and directory specified in `video_path` can be found before beginning video processing. If the file path was found, you can proceed to the video processing initialization and video processing steps. If your file was not found and you should amend your `video_path` before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b65dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 2  # 1 = Lite model, 2 = Full model\n",
    "video_path = \"C:/Users/cosmo/Desktop/Random Scripts/Co-Speech Gesture Automation/Co-Speech-Gesture-Automation/TEST_VIDEOS/5003_I_board_one.MOV\"  # Add your video file path here\n",
    "\n",
    "if os.path.exists(video_path) == True:\n",
    "    print(f\"{video_path} is a valid file. Proceed with processing.\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"{video_path} does not exist. Try adding the entire file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1e2c4",
   "metadata": {},
   "source": [
    "### <strong>Initialization</strong>\n",
    "This part initializes MediaPipe components used in the notebook.\n",
    "\n",
    "There is no need to do anything with this cell before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34058d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe components\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3130a9",
   "metadata": {},
   "source": [
    "### <strong>Keypoint Tracking</strong>\n",
    "The core logic of video processing is performed in this loop.\n",
    "This code block reads in a video file and extracts pose landmarks from each frame of the video using the `mediapipe` and `opencv-python` libraries. The pose landmarks are then stored in a DataFrame along with the corresponding time stamp.\n",
    "\n",
    "This cell can be run without making any changes unless you require tracking for keypoints not specified in this workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa76350",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Processing video at {video_path}\")\n",
    "with mp_holistic.Holistic(static_image_mode=False, model_complexity=MODEL) as holistic:\n",
    "    # Initialize DataFrame to store data\n",
    "    data = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            break\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Append data to list\n",
    "        time_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        \n",
    "        # Dictionary to store data\n",
    "        if results.pose_landmarks is not None:\n",
    "            right_shoulder_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER].x\n",
    "            right_shoulder_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER].y\n",
    "            left_shoulder_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER].x\n",
    "            left_shoulder_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER].y\n",
    "            right_elbow_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW].x\n",
    "            right_elbow_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW].y\n",
    "            left_elbow_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW].x\n",
    "            left_elbow_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW].y\n",
    "            right_wrist_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_WRIST].x\n",
    "            right_wrist_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_WRIST].y\n",
    "            left_wrist_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_WRIST].x\n",
    "            left_wrist_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_WRIST].y\n",
    "            right_eye_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_EYE].x\n",
    "            right_eye_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_EYE].y\n",
    "            left_eye_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EYE].x\n",
    "            left_eye_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EYE].y\n",
    "            nose_x = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].x\n",
    "            nose_y = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].y\n",
    "            \n",
    "            data.append([time_ms, right_shoulder_x, right_shoulder_y, left_shoulder_x, left_shoulder_y, right_elbow_x, right_elbow_y, left_elbow_x, left_elbow_y, right_wrist_x, right_wrist_y, left_wrist_x, left_wrist_y, right_eye_x, right_eye_y, left_eye_x, left_eye_y, nose_x, nose_y])\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cffe5c1",
   "metadata": {},
   "source": [
    "### <strong>Converting the data to a dataframe</strong>\n",
    "Here the keypoint data is converted to a dataframe to preview before writing to a csv file. All keypoint data are saved at this stage of processing, and there is no need to make changes to the articulators listed here; however, later stages of processing will subset the data to a chosen articulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada8edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    \"time_ms\", \n",
    "    \"right_shoulder_x\", \"right_shoulder_y\", \n",
    "    \"left_shoulder_x\", \"left_shoulder_y\", \n",
    "    \"right_elbow_x\", \"right_elbow_y\", \n",
    "    \"left_elbow_x\", \"left_elbow_y\", \n",
    "    \"right_wrist_x\", \"right_wrist_y\", \n",
    "    \"left_wrist_x\", \"left_wrist_y\", \n",
    "    \"right_eye_x\", \"right_eye_y\", \n",
    "    \"left_eye_x\", \"left_eye_y\",\n",
    "    \"nose_x\", \"nose_y\"\n",
    "    ])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691108d0",
   "metadata": {},
   "source": [
    "### <strong>Data Output</strong>\n",
    "Finally, the processed data is stored in a DataFrame and saved as a pickle file or csv file. For testing we recommend using a csv file and then moving to a pickle file for processing larger datasets because pickle files are more efficient for processing and storing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print DataFrame shape\n",
    "print(f\"DataFrame Head: {df.head()}\")\n",
    "\n",
    "# # Save DataFrame as pickle file\n",
    "# pickle_file_name = \"C:/Users/cosmo/Desktop/Random Scripts/Co-Speech Gesture Automation/Co-Speech-Gesture-Automation/MOTION_TRACKING_FILES/\" + os.path.splitext(os.path.basename(video_path))[0] + \"_keypoints.pkl\"\n",
    "# df.to_pickle(pickle_file_name)\n",
    "# print(f\"DataFrame saved as {pickle_file_name}\")\n",
    "\n",
    "# Save DataFrame as CSV file\n",
    "csv_file_name = \"C:/Users/cosmo/Desktop/Random Scripts/Co-Speech Gesture Automation/Co-Speech-Gesture-Automation/MOTION_TRACKING_FILES/\" + os.path.splitext(os.path.basename(video_path))[0] + \"_keypoints.csv\"\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "print(f\"DataFrame saved as {csv_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
