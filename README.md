
# Co-Speech Gesture Automation

## Overview

This repository aims to automate the coding of gestural landmarks aligned with speech. Utilizing the power of MediaPipe for pose estimation and various Python packages, the project offers a streamlined workflow for research and application purposes.

## Technologies Used

- MediaPipe
- Pandas
- NumPy
- OpenCV (cv2)
- Parselmouth
- Matplotlib
- SciPy

## Installation

1. Clone the repository:

   ``` sh
   git clone https://github.com/walterdych/Co-Speech-Gesture-Automation.git
   ```

2. Install the required Python packages:

   ``` sh
   pip install mediapipe pandas numpy opencv-python parselmouth matplotlib scipy
   ```

## Usage

The repository includes different scripts optimized for various environments:

- **Supercomputing Resources**: Optimal for batch processing and extensive data analysis.
- **Local Machines**: For everyday usage and development.
- **Testing**: For experimenting and troubleshooting.

## Additional Features

- High scalability
- Comprehensive documentation
- Versatility in usage (research, development, etc.)

## Contributing

If you'd like to contribute, please fork the repository and create a pull request, or open an issue for discussion.

## License

This project is licensed under the MIT License. See `LICENSE` for more details.
